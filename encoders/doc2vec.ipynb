{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varunsatheesh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'ate', 'dinner', '.'], ['we', 'had', 'a', 'three-course', 'meal', '.'], ['brad', 'came', 'to', 'dinner', 'with', 'us', '.'], ['he', 'loves', 'fish', 'tacos', '.'], ['in', 'the', 'end', ',', 'we', 'all', 'felt', 'like', 'we', 'ate', 'too', 'much', '.'], ['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I ate dinner.\", \n",
    "       \"We had a three-course meal.\", \n",
    "       \"Brad came to dinner with us.\",\n",
    "       \"He loves fish tacos.\",\n",
    "       \"In the end, we all felt like we ate too much.\",\n",
    "       \"We all agreed; it was a magnificent evening.\"]\n",
    "tokenized_sent = []\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/varunsatheesh/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'ate', 'dinner', '.'], tags=[0]),\n",
       " TaggedDocument(words=['we', 'had', 'a', 'three-course', 'meal', '.'], tags=[1]),\n",
       " TaggedDocument(words=['brad', 'came', 'to', 'dinner', 'with', 'us', '.'], tags=[2]),\n",
       " TaggedDocument(words=['he', 'loves', 'fish', 'tacos', '.'], tags=[3]),\n",
       " TaggedDocument(words=['in', 'the', 'end', ',', 'we', 'all', 'felt', 'like', 'we', 'ate', 'too', 'much', '.'], tags=[4]),\n",
       " TaggedDocument(words=['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.'], tags=[5])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x17f6de650>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''\n",
    "\n",
    "## Print model vocabulary\n",
    "model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/lh2dfcrs6y72hrq4c4bdcn4w0000gn/T/ipykernel_95346/3831647823.py:3: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  model.docvecs.most_similar(positive = [test_doc_vector])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.6497181057929993),\n",
       " (2, 0.5994305610656738),\n",
       " (1, 0.5180383920669556),\n",
       " (3, 0.4926862418651581),\n",
       " (4, 0.41835761070251465),\n",
       " (0, 0.27052268385887146)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = word_tokenize(\"I ate dinner\".lower())\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "model.docvecs.most_similar(positive = [test_doc_vector])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
